# Salesforce â†’ Databricks-Style Data Pipeline Simulation

## ğŸ“Š Project Overview
This project simulates a data pipeline that extracts sales data from an API, transforms it using PySpark (Databricks-style), and loads the results into Power BIâ€“ready Excel and Parquet formats.  
The goal is to demonstrate experience with **ETL, Databricks workflows, PySpark, and Delta Lake concepts**.

---

## ğŸ”¹ Workflow
1. **Extract**  
   - Load sales data from a mock API / JSON file.  

2. **Transform** (Databricks Simulation with PySpark)  
   - Clean missing values  
   - Aggregate sales by customer and month  
   - Save results in Parquet (Delta-style format)  

3. **Load**  
   - Export cleaned dataset into Excel for Power BI dashboarding  

---

## ğŸ› ï¸ Tech Used
- Python (pandas, requests, openpyxl)  
- PySpark (for Databricks-like transformations)  
- Parquet files (simulate Delta Lake storage)  
- Power BI (connects to exported Excel dataset)  

---

## ğŸ“‚ Project Structure

azure-databricks-pipeline-simulation/
â”‚â”€â”€ notebooks/
â”‚ â””â”€â”€ sales_pipeline_notebook.py # PySpark ETL notebook
â”‚â”€â”€ data/
â”‚ â””â”€â”€ raw_sales.json # Mock input data
â”‚â”€â”€ outputs/
â”‚ â”œâ”€â”€ cleaned_sales.parquet
â”‚ â””â”€â”€ sales_summary.xlsx
â”‚â”€â”€ pipeline.py # End-to-end Python runner
â”‚â”€â”€ README.md # Documentation


---

## ğŸš€ Next Steps
- Replace mock API with **Salesforce REST API** or Azure SQL  
- Deploy PySpark notebook in **Azure Databricks**  
- Schedule job execution with **Databricks Jobs / Azure Data Factory**  
- Secure credentials using **Azure Key Vault**  

---

## ğŸ”‘ Key Skills Demonstrated
âœ… ETL Pipeline Design  
âœ… Databricks Notebook Workflow  
âœ… PySpark Transformations  
âœ… Delta Lake Simulation (Parquet)  
âœ… Power BI Export & Automation  